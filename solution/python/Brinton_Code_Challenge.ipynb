{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brinton Code Challenge Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I will be using a Notebook for documentation and development. My process for building the solution will be as follows:\n",
    "* I will write ewach section in the notebook.\n",
    "* I will debug and validate in the notebook.\n",
    "* The final results will be deployed into an exacutable service.\n",
    "* All deliverables will be uploaded into GIT. -- add director and link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Definitiions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for data manipulation\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rfc3339 as rfc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_data(df):\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "    ## Build purge and reporting function\n",
    "      print('Missing values found')\n",
    "      ## Purge the missing values\n",
    "      df = df.dropna()\n",
    "    ## Report the missing values\n",
    "      df.to_csv('missing_values.csv')\n",
    "      print('Missing values have been purged and reported in missing_values.csv')\n",
    "    else:\n",
    "      print('No missing values found')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_invalid_shifts_times(df):\n",
    "## Check if the shift is valid\n",
    "  invalid_shifts = df[df['StartTime'] > df['EndTime']]\n",
    "  \n",
    "  if invalid_shifts.shape[0] > 0:\n",
    "    ## Build purge and reporting function for invalid shifts\n",
    "    print('Invalid shifts found', invalid_shifts)\n",
    "  else:\n",
    "    print('No invalid shifts found')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function uses merge instead of a nested loop to find overlapping shifts and return the DF with the overlapping shifts\n",
    "\n",
    "## INPUTS:  DF\n",
    "## OUTPUT: DF with overlapping shifts\n",
    "\n",
    "def find_overlapping_shifts(df):\n",
    "    ## Perform self-join on EmployeeId to compare shifts\n",
    "    overlapping_shifts = df.merge(df, on=\"EmployeeID\", suffixes=(\"_A\", \"_B\"))\n",
    "    ## Filter overlapping shifts\n",
    "    overlapping_shifts = overlapping_shifts[\n",
    "    (overlapping_shifts[\"ShiftID_A\"] < overlapping_shifts[\"ShiftID_B\"]) &  # Avoid duplicate comparisons\n",
    "    (overlapping_shifts[\"StartTime_A\"] < overlapping_shifts[\"EndTime_B\"]) &\n",
    "    (overlapping_shifts[\"StartTime_B\"] < overlapping_shifts[\"EndTime_A\"])\n",
    "    ]\n",
    "    ## Select relevant columns\n",
    "    overlapping_shifts = overlapping_shifts[[\"ShiftID_A\", \"ShiftID_B\", \"EmployeeID\"]]\n",
    "    return overlapping_shifts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeoverlapshifts(resultsdf, overlapdf):\n",
    "    \n",
    "    ## need to add in shift week   \n",
    "    # Combine ShiftID_A and ShiftID_B into a single column\n",
    "    overlapdf_melted = (overlapdf.melt(id_vars=[\"EmployeeID\"], value_vars=[\"ShiftID_A\", \"ShiftID_B\"], value_name=\"ShiftID\"))\n",
    "\n",
    "    # Group shifts into lists per EmployeeID\n",
    "    shifts_per_employee = overlapdf_melted.groupby(\"EmployeeID\")[\"ShiftID\"].unique().reset_index()\n",
    "\n",
    "    # Convert NumPy array to a list for readability\n",
    "    shifts_per_employee[\"ShiftID\"] = shifts_per_employee[\"ShiftID\"].apply(list)\n",
    "    \n",
    "    # Merge shift lists into resultsdf\n",
    "    resultsdf = resultsdf.merge(shifts_per_employee, on=\"EmployeeID\", how=\"left\")\n",
    "\n",
    "    # Rename column for clarity\n",
    "    resultsdf.rename(columns={\"ShiftID\": \"InvalidShifts\"}, inplace=True)\n",
    "\n",
    "    # Fill missing values with empty lists\n",
    "    resultsdf[\"InvalidShifts\"] = resultsdf[\"InvalidShifts\"].apply(lambda x: x if isinstance(x, list) else [])\n",
    "    \n",
    "    return resultsdf\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropinvalidshifts(df, overlapdf):\n",
    "    # Get ShiftIDs to drop\n",
    "    invalid_shifts = set(overlapdf[\"ShiftID_A\"]).union(set(overlapdf[\"ShiftID_B\"]))\n",
    "    print('number overlap shifts:', len(invalid_shifts))\n",
    "\n",
    "    # Drop invalid shifts\n",
    "    print('number of shifts before drop:', df.shape[0])\n",
    "    df = df[~df[\"ShiftID\"].isin(invalid_shifts)]\n",
    "    print('number of shifts after drop:', df.shape[0])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_hours_worked(df):\n",
    "  # Calculate hours worked per shift\n",
    "  df[\"HoursWorked\"] = (df[\"EndTime\"] - df[\"StartTime\"]).dt.total_seconds() / 3600\n",
    "\n",
    "  # Get StartOfWeek for each shift (Monday of that week)\n",
    "  df[\"StartOfWeek\"] = df['StartTime'].dt.to_period('W').dt.start_time\n",
    "\n",
    "  # Group by EmployeeID and StartOfWeek\n",
    "  weekly_hours = df.groupby([\"EmployeeID\", \"StartOfWeek\"])[\"HoursWorked\"].sum().reset_index()\n",
    "\n",
    "  # Split into RegularHours and OvertimeHours\n",
    "  weekly_hours[\"RegularHours\"] = weekly_hours[\"HoursWorked\"].clip(upper=40)  # Max 40 regular hours\n",
    "  weekly_hours[\"OvertimeHours\"] = (weekly_hours[\"HoursWorked\"] - 40).clip(lower=0)  # Overtime is extra hours\n",
    "\n",
    "  # Drop total hours (not needed)\n",
    "  weekly_hours.drop(columns=[\"HoursWorked\"], inplace=True)\n",
    "\n",
    "  return weekly_hours\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Main execution of the program\n",
    "### This code will call funcitons for dependent services.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[97]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m## Open the json data file.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdataset.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      3\u001b[39m     data = json.load(f)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m## Create a dataframe out of input source\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/IPython/core/interactiveshell.py:325\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    320\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'dataset.json'"
     ]
    }
   ],
   "source": [
    "## Open the json data file.\n",
    "with open('dataset.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "## Create a dataframe out of input source\n",
    "df = pd.DataFrame(data)\n",
    "## df.head()\n",
    "## Define output source data frame\n",
    "resultdf = pd.DataFrame(columns=['EmployeeID', 'StartOfWeek', 'RegularHours', 'OvertimeHours'])\n",
    "\n",
    "\n",
    "## Process the data for errors\n",
    "## Check for missing values\n",
    "check_missing_data(df)\n",
    "\n",
    "## convert RFC3339 to timestamp in pandas.\n",
    "## This is my first experience with the RFC33339 format, So I am not sure of the best practice to using this format.  I am converting for the sake of time.\n",
    "df['StartTime'] = pd.to_datetime(df['StartTime'])\n",
    "df['EndTime'] = pd.to_datetime(df['EndTime'])\n",
    "\n",
    "## Check for invalid shifts\n",
    "check_invalid_shifts_times(df)\n",
    "\n",
    "## sort df by employeeID and StartTime\n",
    "df = df.sort_values(by=['EmployeeID', 'StartTime']) \n",
    "\n",
    "## build base of reults dataframe\n",
    "##resultdf['EmployeeID'] = df['EmployeeID'].unique()\n",
    "##resultdf['StartOfWeek'] = df['StartTime'].dt.to_period('W').dt.start_time\n",
    "\n",
    "## find shifts that overlap\n",
    "dfoverlap = find_overlapping_shifts(df)\n",
    "\n",
    "## add overlap shifts to results dataframe\n",
    "##resultdf = mergeoverlapshifts(resultdf, dfoverlap)\n",
    "\n",
    "## Drop the invalid shifts from the dataframe\n",
    "df = dropinvalidshifts(df, dfoverlap)\n",
    "\n",
    "## Calculate hours worked and assign to the output dataframe\n",
    "hoursworked = assign_hours_worked(df)\n",
    "\n",
    "## merge hours worked to the output dataframe\n",
    "resultdf = mergeoverlapshifts(hoursworked, dfoverlap)\n",
    "\n",
    "## Write the output to a csv file\n",
    "resultdf.to_csv('output.csv', index=False)\n",
    "print('Output file created')   \n",
    "\n",
    "## Missing validations for the following:\n",
    "## 1.  Check for shifts that carry over sunday intot he next week\n",
    "## 2.  Need to clean up the Invalid Shifts list to remove the data type in the list.\n",
    "## 3.  I never touched on the day light savings issue. \n",
    "## 4.  There is no test suit to validate the code. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
